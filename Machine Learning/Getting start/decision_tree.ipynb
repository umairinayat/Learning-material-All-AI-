{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is For Entropy, Gini, and Information Gain Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Selection Measures:\n",
    "Construction of Decision Tree: A tree can be “learned” by splitting the source set into subsets based on Attribute Selection Measures. Attribute selection measure (ASM) is a criterion used in decision tree algorithms to evaluate the usefulness of different attributes for splitting a dataset. The goal of ASM is to identify the attribute that will create the most homogeneous subsets of data after the split, thereby maximizing the information gain. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node all has the same value of the target variable, or when splitting no longer adds value to the predictions. The construction of a decision tree classifier does not require any domain knowledge or parameter setting and therefore is appropriate for exploratory knowledge discovery. Decision trees can handle high-dimensional data.\n",
    "\n",
    "Entropy:\n",
    "Entropy is the measure of the degree of randomness or uncertainty in the dataset. In the case of classifications, It measures the randomness based on the distribution of class labels in the dataset.\n",
    "\n",
    "The entropy for a subset of the original dataset having K number of classes for the ith node can be defined as:\n",
    "\n",
    "\n",
    "![alt text](https://www.clairvoyant.ai/hs-fs/hubfs/1_Mag0PsD0O6QeNSXBb6BTzQ.png?width=700&height=42&name=1_Mag0PsD0O6QeNSXBb6BTzQ.png)\n",
    "\n",
    "\n",
    "Where,\n",
    "\n",
    "`S` is the dataset sample.\\\n",
    "`k` is the particular class from K classes\\\n",
    "`p(k)` is the proportion of the data points that belong to class k to the total number of data points in dataset sample S.\n",
    "\n",
    "Here `p(i,k)` should not be equal to zero.\n",
    "\n",
    "### Important points related to Entropy:\n",
    "\n",
    "The entropy is 0 when the dataset is completely homogeneous, meaning that each instance belongs to the same class. It is the lowest entropy indicating no uncertainty in the dataset sample.\n",
    "when the dataset is equally divided between multiple classes, the entropy is at its maximum value. Therefore, entropy is highest when the distribution of class labels is even, indicating maximum uncertainty in the dataset sample.\n",
    "Entropy is used to evaluate the quality of a split. The goal of entropy is to select the attribute that minimizes the entropy of the resulting subsets, by splitting the dataset into more homogeneous subsets with respect to the class labels.\n",
    "The highest information gain attribute is chosen as the splitting criterion (i.e., the reduction in entropy after splitting on that attribute), and the process is repeated recursively to build the decision tree.\n",
    "\n",
    "\n",
    "### Gini Impurity or index:\n",
    "Gini Impurity is a score that evaluates how accurate a split is among the classified groups. The Gini Impurity evaluates a score in the range between 0 and 1, where 0 is when all observations belong to one class, and 1 is a random distribution of the elements within classes. In this case, we want to have a Gini index score as low as possible. Gini Index is the evaluation metric we shall use to evaluate our Decision Tree Model.\n",
    "\n",
    "\n",
    "![alt text](https://www.clairvoyant.ai/hs-fs/hubfs/1_bOOmVYHSwWcPoXZKSLc2Xw.png?width=700&height=43&name=1_bOOmVYHSwWcPoXZKSLc2Xw.png)\n",
    "\n",
    "\n",
    "\n",
    "Here,\n",
    "\n",
    "`pi` is the proportion of elements in the set that belongs to the ith category.\n",
    "\n",
    "### Information Gain:\n",
    "\n",
    "\n",
    "Information gain measures the reduction in entropy or variance that results from splitting a dataset based on a specific property. It is used in decision tree algorithms to determine the usefulness of a feature by partitioning the dataset into more homogeneous subsets with respect to the class labels or target variable. The higher the information gain, the more valuable the feature is in predicting the target variable. \n",
    "\n",
    "The information gain of an attribute A, with respect to a dataset S, is calculated as follows:\n",
    "\n",
    "![alt text](https://www.clairvoyant.ai/hs-fs/hubfs/1_5Bzoc6n44YXGAZtmEWCi9Q.png?width=700&height=51&name=1_5Bzoc6n44YXGAZtmEWCi9Q.png)\n",
    "\n",
    "Where,\n",
    "\n",
    "\n",
    "`A` is the specific attribute or class label\\\n",
    "`|H|` is the entropy of dataset sample S\\\n",
    "`|HV|` is the number of instances in the subset S that have the value v for attribute A.\\\n",
    "\n",
    "\n",
    "Information gain measures the reduction in entropy or variance achieved by partitioning the dataset on attribute A. The attribute that maximizes information gain is chosen as the splitting criterion for building the decision tree.\n",
    "\n",
    "Information gain is used in both classification and regression decision trees. In classification, entropy is used as a measure of impurity, while in regression, variance is used as a measure of impurity. The information gain calculation remains the same in both cases, except that entropy or variance is used instead of entropy in the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement math (from versions: none)\n",
      "ERROR: No matching distribution found for math\n"
     ]
    }
   ],
   "source": [
    "%pip install math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example Dataset \n",
    "#Let's say we have a data with two classes, A and B.\n",
    "#Suppose we have 10 elements, 4 are from class A and 6 are from class B.\n",
    "\n",
    "#Number of element in each class \n",
    "n_A = 4\n",
    "n_B = 6\n",
    "\n",
    "total = n_A + n_B   #Total number of elements\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of class A is: 0.4\n",
      "The probability of class B is: 0.6\n"
     ]
    }
   ],
   "source": [
    "#let calculate the probability of each class\n",
    "p_A = n_A / total\n",
    "p_B = n_B / total  \n",
    "\n",
    "#print the proprotions\n",
    "print(\"The probability of class A is:\", p_A)\n",
    "print(\"The probability of class B is:\", p_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of the dataset is: 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "#Entropy calculation\n",
    "#Entropy is a measure of the uncertainty of a random variable.\n",
    "#It is a measure of the impurity in a set of examples.\n",
    "\n",
    "#Entropy is calculated as follows:\n",
    "\n",
    "entropy= -p_A * math.log2(p_A) - p_B * math.log2(p_B)\n",
    "print(\"The entropy of the dataset is:\", entropy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gini impurity of the dataset is: 0.48\n"
     ]
    }
   ],
   "source": [
    "#Gini Impurity\n",
    "#Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n",
    "\n",
    "gini = 1 - (p_A**2 + p_B**2)\n",
    "print(\"The Gini impurity of the dataset is:\", gini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information gain of the dataset is: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Information Gain\n",
    "#Information gain is the measure of the reduction in entropy or impurity after the dataset is split on an attribute.\n",
    "\n",
    "#Information gain is calculated as follows:\n",
    "#Assuming a split on some feature divides the dataset into two subsets\n",
    "#Subset 1: 2 elements of A, 3 of B\n",
    "#Subset 2: 2 elements of A, 3 of B\n",
    "\n",
    "#Entropy and size for each subset \n",
    "\n",
    "n_1_A, n_1_B = 2, 3\n",
    "n_2_A, n_2_B = 2, 3\n",
    "\n",
    "p_1_A = n_1_A / (n_1_A + n_1_B)\n",
    "p_1_B = n_1_B / (n_1_A + n_1_B)\n",
    "entropy_1 = -p_1_A * math.log2(p_1_A) - p_1_B * math.log2(p_1_B)\n",
    "\n",
    "p_2_A = n_2_A / (n_2_A + n_2_B)\n",
    "p_2_B = n_2_B / (n_2_A + n_2_B)\n",
    "entropy_2 = -p_2_A * math.log2(p_2_A) - p_2_B * math.log2(p_2_B)\n",
    "\n",
    "#Calculate the information gain\n",
    "#Information gain is the difference between the entropy of the original dataset and the entropy of the two subsets\n",
    "\n",
    "info_gain = entropy - ( (n_1_A + n_1_B) / total * entropy_1 + (n_2_A + n_2_B) / total * entropy_2)\n",
    "\n",
    "print(\"The information gain of the dataset is:\", info_gain)\n",
    "\n",
    "\n",
    "###Based on Our Example dataset with two classes (A and B) we have calculated the following values:\n",
    "\n",
    "## 1. Entropy: The calculated entropy of the dataset is 0.9709505944546686.\n",
    "# This value indicates a moderate level of impurity in the dataset, Considering that it ranges from 0 to 1, \n",
    "#  where 0 means that the dataset is completely pure and 1 means that the dataset is completely impure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Example \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a flowchart-like tree structure where each internal node denotes the feature, branches denote the rules and the leaf nodes denote the result of the algorithm. It is a versatile supervised machine-learning algorithm, which is used for both classification and regression problems. It is one of the very powerful algorithms. And it is also used in Random Forest to train on different subsets of training data, which makes random forest one of the most powerful algorithms in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is a type of supervised learning algorithm that is mostly used in classification problems. It works for both continuous as well as categorical output variables. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.\n",
    "\n",
    "\n",
    "\n",
    "### Decision Tree Terminologies\n",
    "\n",
    "Some of the common Terminologies used in Decision Trees are as follows:\n",
    "\n",
    "* Root Node: It is the topmost node in the tree,  which represents the complete dataset. It is the starting point of the decision-making process.\n",
    "* Decision/Internal Node: A node that symbolizes a choice regarding an input feature. Branching off of internal nodes connects them to leaf nodes or other internal nodes.\n",
    "* Leaf/Terminal Node: A node without any child nodes that indicates a class label or a numerical value.\n",
    "* Splitting: The process of splitting a node into two or more sub-nodes using a split criterion and a selected feature.\n",
    "* Branch/Sub-Tree: A subsection of the decision tree starts at an internal node and ends at the leaf nodes.\n",
    "* Parent Node: The node that divides into one or more child nodes.\n",
    "* Child Node: The nodes that emerge when a parent node is split.\n",
    "* Impurity: A measurement of the target variable’s homogeneity in a subset of data. It refers to the degree of randomness or uncertainty in a set of examples. The Gini index and entropy are two commonly used impurity measurements in decision trees for classifications task \n",
    "* Variance: Variance measures how much the predicted and the target variables vary in different samples of a dataset. It is used for regression problems in decision trees. Mean squared error, Mean Absolute Error, friedman_mse, or Half Poisson deviance are used to measure the variance for the regression tasks in the decision tree.\n",
    "* Information Gain: Information gain is a measure of the reduction in impurity achieved by splitting a dataset on a particular feature in a decision tree. The splitting criterion is determined by the feature that offers the greatest information gain, It is used to determine the most informative feature to split on at each node of the tree, with the goal of creating pure subsets\n",
    "* Pruning: The process of removing branches from the tree that do not provide any additional information or lead to overfitting.\n",
    "\n",
    "### Advantages of Decision Tree:\n",
    "1. Simple to understand and to interpret. Trees can be visualised.\n",
    "2. Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed.\n",
    "3. The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "4. Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable.\n",
    "5. Able to handle multi-output problems.\n",
    "6. Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "7. Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "8. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "\n",
    "### Disadvantages of Decision Tree:\n",
    "1. Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "2. Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "3. The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree.\n",
    "4. There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "5. Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "![alt text](https://media.geeksforgeeks.org/wp-content/uploads/20230424141727/Decision-Tree.webp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data set \n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop deck column\n",
    "df.drop('deck', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male  embark_town alive  alone  \n",
       "0    man        True  Southampton    no  False  \n",
       "1  woman       False    Cherbourg   yes  False  \n",
       "2  woman       False  Southampton   yes   True  \n",
       "3  woman       False  Southampton   yes  False  \n",
       "4    man        True  Southampton    no   True  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values of age and fare using mediam\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[['age', 'fare']] = imputer.fit_transform(df[['age', 'fare']])\n",
    "\n",
    "#impute missing value of embark and embarked_town using mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['embarked', 'embark_town']] = imputer.fit_transform(df[['embarked', 'embark_town']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived       0\n",
       "pclass         0\n",
       "sex            0\n",
       "age            0\n",
       "sibsp          0\n",
       "parch          0\n",
       "fare           0\n",
       "embarked       0\n",
       "class          0\n",
       "who            0\n",
       "adult_male     0\n",
       "embark_town    0\n",
       "alive          0\n",
       "alone          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   survived     891 non-null    int64  \n",
      " 1   pclass       891 non-null    int64  \n",
      " 2   sex          891 non-null    int32  \n",
      " 3   age          891 non-null    float64\n",
      " 4   sibsp        891 non-null    int64  \n",
      " 5   parch        891 non-null    int64  \n",
      " 6   fare         891 non-null    float64\n",
      " 7   embarked     891 non-null    int32  \n",
      " 8   class        891 non-null    int32  \n",
      " 9   who          891 non-null    int32  \n",
      " 10  adult_male   891 non-null    bool   \n",
      " 11  embark_town  891 non-null    int32  \n",
      " 12  alive        891 non-null    int32  \n",
      " 13  alone        891 non-null    bool   \n",
      "dtypes: bool(2), float64(2), int32(6), int64(4)\n",
      "memory usage: 64.5 KB\n"
     ]
    }
   ],
   "source": [
    "# Encode the categorical and object variables using for loop and labelencoder \n",
    "le = LabelEncoder()\n",
    "for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "df.info()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass  sex   age  sibsp  parch     fare  embarked  class  who  \\\n",
       "0         0       3    1  22.0      1      0   7.2500         2      2    1   \n",
       "1         1       1    0  38.0      1      0  71.2833         0      0    2   \n",
       "2         1       3    0  26.0      0      0   7.9250         2      2    2   \n",
       "3         1       1    0  35.0      1      0  53.1000         2      0    2   \n",
       "4         0       3    1  35.0      0      0   8.0500         2      2    1   \n",
       "\n",
       "   adult_male  embark_town  alive  alone  \n",
       "0        True            2      0  False  \n",
       "1       False            0      1  False  \n",
       "2       False            2      1   True  \n",
       "3       False            2      1  False  \n",
       "4        True            2      0   True  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splict the data into X and Y\n",
    "X= df.drop(['survived','alive'], axis= 1)\n",
    "y= df['survived']\n",
    "# split the data into train and test \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Convert the target variable to discrete classes\n",
    "le = LabelEncoder()\n",
    "#y_train = le.fit_transform(y_train)\n",
    "#y_test = le.transform(y_test)\n",
    "\n",
    "# Create and train the model with the fixed target variable\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=50, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the fixed model\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8212290502793296\n",
      "[[92 13]\n",
      " [19 55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85       105\n",
      "           1       0.81      0.74      0.77        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.82      0.81      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the decision tree classifier \n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(model, out_file='./Saved_model/Decision_tree_02.dot', feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True, special_characters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run Decision Tree classifier on the famous Iris dataset and calculate the Entropy, Gini, and Information Gain for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
