{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE Bayes algorithm\n",
    "\n",
    "Naive Bayes Algorithm is a classification algorithm based on Bayes Theorem. It is called naive because it assumes that the features in a dataset are independent of each other. This assumption is not true in real life but it simplifies the computation and gives good results in most of the cases.\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "Bayes Theorem is a mathematical formula used for calculating conditional probability. It is defined as:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "where A and B are events and P(B) != 0\n",
    "\n",
    "## Naive Bayes Algorithm\n",
    "\n",
    "Naive Bayes Algorithm is based on Bayes Theorem. It is defined as:\n",
    "\n",
    "$$P(y|x_1,x_2,...,x_n) = \\frac{P(x_1,x_2,...,x_n|y)P(y)}{P(x_1,x_2,...,x_n)}$$\n",
    "\n",
    "where y is the class variable and x1, x2, ..., xn are the features.\n",
    "\n",
    "The algorithm assumes that the features are independent of each other. So, the above equation can be written as:\n",
    "\n",
    "$$P(y|x_1,x_2,...,x_n) = \\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1,x_2,...,x_n)}$$\n",
    "\n",
    "The denominator is constant for a given input. So, the equation can be written as:\n",
    "\n",
    "$$P(y|x_1,x_2,...,x_n) \\propto P(x_1|y)P(x_2|y)...P(x_n|y)P(y)$$\n",
    "\n",
    "The class with the highest probability is the output of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Naive Bayes Algorithm\n",
    "\n",
    "There are three types of Naive Bayes Algorithm:\n",
    "\n",
    "1. Gaussian Naive Bayes: It is used when the features are continuous. It assumes that the features follow a normal distribution.\n",
    "\n",
    "2. Multinomial Naive Bayes: It is used when the features are discrete. It is used for text classification.\n",
    "\n",
    "3. Bernoulli Naive Bayes: It is used when the features are binary.\n",
    "\n",
    "## Steps to implement Naive Bayes Algorithm\n",
    "\n",
    "1. Load the dataset\n",
    "2. Split the dataset into training and testing sets\n",
    "3. Initialize the parameters\n",
    "4. Calculate the prior probabilities\n",
    "5. Calculate the likelihood\n",
    "6. Calculate the posterior probabilities\n",
    "7. Make predictions\n",
    "8. Evaluate the model\n",
    "\n",
    "## Advantages of Naive Bayes Algorithm\n",
    "\n",
    "**1-Simplicity** ( straightforward to implement and understand )\n",
    "\n",
    "**2-Efficiency** ( requires a small amount of training data )\n",
    "\n",
    "**3-Speed** ( very fast, making them suitable for real-time prediction )\n",
    "\n",
    "**4-Good performance** ( often performs well in multi-class prediction )\n",
    "\n",
    "**5-Works well with high-dimensional data** ( performs well even when the number of features is large )\n",
    "\n",
    "\n",
    "1. It is simple and easy to implement\n",
    "2. It is fast\n",
    "3. It gives good results in most of the cases\n",
    "4. It can be used for both binary and multiclass classification\n",
    "\n",
    "## Disadvantages of Naive Bayes Algorithm\n",
    "\n",
    "**Feature Independence:** Naive Bayes Algorithm assumes that the features are independent of each other. This assumption is not true in real life. So, the algorithm may not give accurate results in some cases.\n",
    "\n",
    "**Data Scarcity:** Naive Bayes Algorithm does not work well with small datasets. It requires a large amount of data to give accurate results.\n",
    "\n",
    "**Imbalanced Datasets:** Naive Bayes Algorithm does not work well with imbalanced datasets. It gives more weight to the majority class and less weight to the minority class.\n",
    "\n",
    "**Non-Linear Data:** Naive Bayes Algorithm does not work well with non-linear data. It assumes that the features are linearly independent of each other. So, if the features are non-linearly dependent on each other, the algorithm may not give accurate results.\n",
    "\n",
    "**Highly Correlated Features:** Naive Bayes Algorithm does not work well with highly correlated features. It assumes that the features are independent of each other. So, if the features are highly correlated, the algorithm may not give accurate results.\n",
    "Missing Values: Naive Bayes Algorithm does not work well with missing values. It requires complete data to give accurate results.\n",
    "\n",
    "\n",
    "1. It assumes that the features are independent of each other\n",
    "2. It does not work well with large datasets\n",
    "3. It does not work well with imbalanced datasets\n",
    "4. It does not work well with missing values\n",
    "5. It does not work well with non-linear data\n",
    "\n",
    "## Applications of Naive Bayes Algorithm\n",
    "\n",
    "1. Email spam detection (e.g. spam or not spam)\n",
    "2. Text classification (e.g. news articles, web pages, etc.)\n",
    "3. Sentiment analysis (e.g. movie reviews, product reviews, etc.)\n",
    "4. Medical diagnosis (e.g. cancer or not cancer)\n",
    "5. Credit scoring (e.g. good credit or bad credit)\n",
    "6. Recommendation systems (e.g. movie recommendations, product recommendations, etc.)\n",
    "7. Fraud detection (e.g. fraudulent or not fraudulent)\n",
    "8. Weather prediction ( e.g. rainy or sunny day)\n",
    "9. Face recognition \n",
    "10. Handwriting recognition \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five types of NB models under the scikit-learn library:\n",
    "\n",
    "`Gaussian Naive Bayes:` gaussiannb is used in classification tasks and it assumes that feature values follow a gaussian distribution.\n",
    "\n",
    "`Multinomial Naive Bayes:` It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\\\n",
    "`Bernoulli Naive Bayes:` The binomial model is useful if your feature vectors are boolean (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\\\n",
    "`Complement Naive Bayes:` It is an adaptation of Multinomial NB where the complement of each class is used to calculate the model weights. So, this is suitable for imbalanced data sets and often outperforms the MNB on text classification tasks.\\\n",
    "`Categorical Naive Bayes:` Categorical Naive Bayes is useful if the features are categorically distributed. We have to encode the categorical variable in the numeric format using the ordinal encoder for using this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# train test split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9777777777777777\n",
      "Confusion Matrix: \n",
      " [[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model initialize\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train the model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# predict the test data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9555555555555556\n",
      "Confusion Matrix: \n",
      " [[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  1 12]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.92      0.92      0.92        13\n",
      "           2       0.92      0.92      0.92        13\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model initialize\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# train the model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# predict the test data\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.28888888888888886\n",
      "Confusion Matrix: \n",
      " [[ 0 19  0]\n",
      " [ 0 13  0]\n",
      " [ 0 13  0]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        19\n",
      "           1       0.29      1.00      0.45        13\n",
      "           2       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.29        45\n",
      "   macro avg       0.10      0.33      0.15        45\n",
      "weighted avg       0.08      0.29      0.13        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# model initialize\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# train the model\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# predict the test data\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
